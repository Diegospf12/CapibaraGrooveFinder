# **Capibara Groove Finder**
**Proyecto 2 & 3 del curso de Base de datos II. Construcci√≥n del √≠ndice invertido textual y multidimensional**

# Team - Group 5
| <a href="https://github.com/anamariaaccilio" target="_blank">**Ana Maria Accilio Villanueva**</a> | <a href="https://github.com/Diegospf12" target="_blank">**Diego Pacheco Ferrel**</a> | <a href="https://github.com/juanpedrovv" target="_blank">**Juan Pedro Vasquez Vilchez**</a> | <a href="https://github.com/LuisEnriqueCortijoGonzales" target="_blank">**Luis Enrique Cortijo Gonzales**</a> | <a href="https://github.com/marceloZS" target="_blank">**Marcelo Mario Zuloeta Salazar**</a> |
| :----------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------: |
| <img src="https://avatars.githubusercontent.com/u/91237434?v=4" alt="drawing" width="200"/> | <img src="https://avatars.githubusercontent.com/u/94090499?v=4" alt="drawing" width="200"/> | <img src="https://avatars.githubusercontent.com/u/83739305?v=4" alt="drawing" width="200"/> | <img src="https://avatars.githubusercontent.com/u/84096868?v=4" alt="drawing" width="200"/> | <img src="https://avatars.githubusercontent.com/u/85197213?v=4" alt="drawing" width="200"/> |

<a name="readme-top"></a>
<details open>
  <summary><h2>Tabla de contenidos:<h2></summary>
  <ul>
    <li><a href="#Introducci√≥n-üñä">Introducci√≥n üñä
      <ul>
        <li><a href="#objetivo-del-proyecto">Objetivo del proyecto</a></li>
        <li><a href="#Dominio-de-datos">Dominio de datos</a></li>
        <li><a href="#Importacia-de-aplicar-indexaci√≥n">Importancia de aplicar indexaci√≥n</a></li>
      </ul>
    </a></li> 
    <li><a href="#Estructura-del-proyecto">Estructura del proyecto</a></li>
    <li><a href="#Backend-(√çndice-Invertido)">Backend (√çndice Invertido)</a></li>
    <li><a href="#Backend-(√çndice-Multidimensional)">Backend (√çndice Multidimensional)</a></li>
    <li><a href="#Frontend-(GUI)">Frontend (GUI)</a></li>
    <li><a href="#¬øC√≥mo-se-construye-el-√≠ndice-invertido-en-PostgreSQL?">¬øC√≥mo se construye el √≠ndice invertido en PostgreSQL?</a></li>
    <li><a href="#Experimentaci√≥n">Experimentaci√≥n</a></li>
    <li><a href="#conclusiones">Conclusiones</a></li>
    <li><a href="#referencias-bibliogr√°ficas">Referencias bibliogr√°ficas</a></li>
</details>

<hr>

# Introducci√≥n

## Objetivo del proyecto
El presente proyecto tiene como objetivo desarrollar estas estructuras de datos de manera eficiente, con motivo de realizar b√∫squedas r√°pidas en un conjunto de documentos.
Con respecto al √≠ndice invertido textual, se utiliza para asociar t√©rminos de consulta con los documentos que los contienen ya que mejora la velocidad y precisi√≥n del retorno de informaci√≥n, lo que facilita la recuperaci√≥n eficiente de documentos relevantes en funci√≥n de los t√©rminos de b√∫squeda.
En cuanto al √≠ndice multidimensional, se utilizar para representar caracter√≠sticas tanto de texto como de audio, lo que permite realizar consultas que involucren m√∫ltiples dimensiones, como la similitud de texto y audio en funci√≥n de diferentes atributos.

## Dominio de datos
La base de datos utilizada es la Audio features and lyrics of Spotify songs, con al rededor de 18000 canciones con los campos:

|    **Campo**    |
|:---------------:|
| ```track_id```        | 
| ```track_name```  | 
| ```track_artist``` | 
| ```lyrics``` | 
| ```track_popularity``` | 
| ```track_album_id``` |
| ```track_album_name``` | 
| ```track_album_release_date``` |
| ```playlist_name``` |
| ```playlist_id``` |
| ```playlist_genre``` |
| ```playlist_subgenre``` |
| ```danceability``` |
| ```enery``` | 
| ```key``` | 
| ```loudness``` | 
| ```mode``` | 
| ```speechness``` | 
| ```acousticness``` | 
| ```instrumentalness``` | 
| ```liveness``` |
| ```valence``` |
| ```tempo``` |
| ```duration_ms``` |
| ```language``` |

## Importancia de aplicar indexaci√≥n
La indexaci√≥n es fundamental por las siguientes razones:
1. Recuperaci√≥n eficiente de informaci√≥n: La indexaci√≥n permite realizar b√∫squedas r√°pidas en grandes conjuntos de datos, lo que es esencial para la recuperaci√≥n eficiente de documentos relevantes en funci√≥n de consultas de texto y audio.

2. Optimizaci√≥n de consultas: Al indexar los datos, se pueden optimizar las consultas para que se ejecuten de manera m√°s eficiente, lo que es crucial para aplicaciones en tiempo real, como la recuperaci√≥n de informaci√≥n en sistemas de b√∫squeda.

3. Reducci√≥n de la complejidad computacional: La indexaci√≥n puede reducir la complejidad computacional de las operaciones de b√∫squeda y an√°lisis de datos, lo que es importante para garantizar un rendimiento √≥ptimo del sistema, especialmente en aplicaciones que manejan grandes vol√∫menes de informaci√≥n.

# Estructura del proyecto
El algoritmo SPIMI (Single-Pass In-Memory Indexing) es un m√©todo utilizado para construir un √≠ndice invertido durante el proceso de indexaci√≥n de grandes conjuntos de datos. A diferencia de algunos algoritmos de indexaci√≥n que requieren m√∫ltiples pasadas sobre los datos, SPIMI realiza la construcci√≥n del √≠ndice en una sola pasada a trav√©s de los datos.

![spimi1](https://github.com/marceloZS/Full-Text-Search_Project2/blob/main/imagenes/spimi1.jpg)

Se calcula una sola vez la longitud de cada documento, y se lee el documento de entrada uno a uno (por ejemplo, un documento de texto) y se tokeniza en t√©rminos individuales. Cada t√©rmino se procesa por separado.

![spimi2](https://github.com/marceloZS/Full-Text-Search_Project2/blob/main/imagenes/spimi2.jpeg)


# Backend (√çndice Invertido)
El archivo inverted_index.py contiene la implementaci√≥n del √≠ndice invertido utilizando el algoritmo SPIMI (Single-Pass In-Memory Indexing). El algoritmo SPIMI divide el proceso de indexaci√≥n en bloques m√°s peque√±os para manejar grandes vol√∫menes de datos de manera eficiente.

## Construcci√≥n del √≠ndice invertido en memoria secundaria

```python

def spimi_invert(self):
    docs, total_terms = self.process_all()
    block_number = 0
    terms_processed = 0
    total_blocks = self.calcular_bloques(total_terms)
    terms_processed_per_block = math.ceil(total_terms/total_blocks)
    local_index = {}
    
    for i in docs:
        doc = i
        for term in doc["terms"]:
            if term not in local_index:
                local_index[term] = [{"id": doc["id"], "tf": 1}]
            else:
                # Comprueba si el id del documento ya existe para este t√©rmino
                doc_found = False
                for doc_i in local_index[term]:
                    if doc_i["id"] == doc["id"]:
                        doc_i["tf"] += 1
                        doc_found = True
                        break
                if not doc_found:
                    # Si el id del documento no se encuentra despu√©s de la iteraci√≥n completa, agrega un nuevo documento
                    local_index[term].append({"id": doc["id"], "tf": 1})
            terms_processed += 1
            if(terms_processed >= terms_processed_per_block):
                sorted_dictionary = collections.OrderedDict(sorted(local_index.items()))
                os.makedirs('local_indexes', exist_ok=True)
                with open(f'local_indexes/block_{block_number}.json', 'w') as file:
                    json.dump(sorted_dictionary, file)
                local_index.clear()
                block_number += 1
                terms_processed = 0

    # Comprueba si quedan t√©rminos en el √∫ltimo bloque
    if local_index:
        sorted_dictionary = collections.OrderedDict(sorted(local_index.items()))
        os.makedirs('local_indexes', exist_ok=True)
        with open(f'local_indexes/block_{block_number}.json', 'w') as file:
            json.dump(sorted_dictionary, file)
    self.binary_merge(total_blocks)

```

Iremos por partes:

```python
def spimi_invert(self):
    docs, total_terms = self.process_all()
    block_number = 0
    terms_processed = 0
    total_blocks = self.calcular_bloques(total_terms)
    terms_processed_per_block = math.ceil(total_terms/total_blocks)
    local_index = {}
```

- `process_all()`
    
    ```python
    def process_all(self):
        docs = []
        total_terms = 0
        for index, row in data.iterrows():
            doc = self.preprocess(row)
            total_terms += len(doc["terms"]) #Se toman en cuenta los terminos incluso esten repetidos, ya que en el merge en el peor de los casos ninguno de los terminos se repite, por lo que el tama√±o de cada bloque deber√≠a ser lo suficientemente grande para que quepan 2^n terminos 
            docs.append(doc)
        return docs, total_terms
    ```
    
    - Itera cada uno de los documentos (rows) de nuestro csv (data textual)
        - Las preprocesa:
            - La tokeniza
            - Elimina las stepwords
            - Realiza Stemming
            - Finalmente el output ser√≠a una lista con cada una de las palabras ya preprocesadas
        - Agregamos en una variable la cantidad de t√©rminos en el documento.
        - Finalmente agregamos nuestro documento a la lista de documentos.
- `calcular_bloques(total_terms)`
    
    ```python
    def calcular_bloques(self, total_terms):
            total_bloques = total_terms // self.block_limit
            bloques_potencia_de_dos = 1
            while bloques_potencia_de_dos * 2 <= total_bloques:
                bloques_potencia_de_dos *= 2
            return bloques_potencia_de_dos
    ```
    
    El par√°metro¬†`total_terms`¬†representa el n√∫mero total de t√©rminos. La funci√≥n divide este n√∫mero por el l√≠mite de bloques (`self.block_limit`) para determinar cu√°ntos bloques se pueden crear. Luego, utiliza un bucle while para encontrar la potencia de dos m√°s grande que sea menor o igual al n√∫mero de bloques calculado anteriormente.
    
    Finalmente, la funci√≥n devuelve el n√∫mero de bloques que se pueden crear, que es la potencia de dos encontrada en el bucle while.
    

```python
	for i in docs:
	    doc = i
	    for term in doc["terms"]:
		if term not in local_index:
		    local_index[term] = [{"id": doc["id"], "tf": 1}]
		else:
		    # Comprueba si el id del documento ya existe para este t√©rmino
		    doc_found = False
		    for doc_i in local_index[term]:
			if doc_i["id"] == doc["id"]:
			    doc_i["tf"] += 1
			    doc_found = True
			    break
		    if not doc_found:
			# Si el id del documento no se encuentra despu√©s de la iteraci√≥n completa, agrega un nuevo documento
			local_index[term].append({"id": doc["id"], "tf": 1})
		terms_processed += 1
```

Construimos un √≠ndice invertido en RAM llamado¬†`local_index`¬†a partir de una lista de documentos. El √≠ndice almacena los t√©rminos presentes en los documentos y para cada t√©rmino, mantiene una lista de documentos asociados con su respectivo contador de frecuencia. Si un t√©rmino ya est√° presente en el √≠ndice, se actualiza el contador de frecuencia para el documento correspondiente. Si el t√©rmino no est√° presente, se agrega una nueva entrada en el √≠ndice con el documento y su contador de frecuencia inicializado en 1.

```python
	#Dentro del for term in doc["terms"]:
		if(terms_processed >= terms_processed_per_block):
		    sorted_dictionary = collections.OrderedDict(sorted(local_index.items()))
		    os.makedirs('local_indexes', exist_ok=True)
		    with open(f'local_indexes/block_{block_number}.json', 'w') as file:
			json.dump(sorted_dictionary, file)
		    local_index.clear()
		    block_number += 1
		    terms_processed = 0
```

Esta parte del c√≥digo se encarga de guardar los √≠ndices locales en archivos separados (Bloques) cuando se alcanza un cierto n√∫mero de t√©rminos procesados. Esto ayuda a dividir el proceso de indexaci√≥n en bloques m√°s peque√±os y facilita la gesti√≥n de los datos, permitiendo que la RAM no llegue a su capacidad m√°xima al almacenar el √≠ndice invertido local.

```python
#Fuera de for i in docs:
    if local_index:
        sorted_dictionary = collections.OrderedDict(sorted(local_index.items()))
        os.makedirs('local_indexes', exist_ok=True)
        with open(f'local_indexes/block_{block_number}.json', 'w') as file:
            json.dump(sorted_dictionary, file)
    self.binary_merge(total_blocks)
```

Verifica si el diccionario¬†`local_index`¬†no est√° vac√≠o. Si no lo est√°, ordena el diccionario, crea un directorio si no existe, y guarda el diccionario ordenado en un archivo JSON (Bloque).

Finalmente, ya que tenemos todos nuestros bloque llamamos a `binary_merge(total_blocks)`, que ser√° explicado a continuaci√≥n:

```python
def binary_merge(self, n_blocks):
    total_blocks = n_blocks
    merge_size = 1
    step = 0
    while merge_size < total_blocks:
        curr_block = 0
        while curr_block < total_blocks:
            next_block = curr_block + merge_size
            if next_block >= total_blocks:
                next_block = curr_block
                curr_block -= merge_size

            folder_name = 'local_indexes' if step == 0 else f'Pasada_{step}'
            with open(f'{folder_name}/block_{curr_block}.json', 'r') as file1:
                index1 = json.load(file1)
            with open(f'{folder_name}/block_{next_block}.json', 'r') as file2:
                index2 = json.load(file2)

            for key, value in index2.items():
                if key in index1:
                    index1[key].extend(value)
                else:
                    index1[key] = value

            merged_dict = index1

            os.makedirs(f'Pasada_{step+1}', exist_ok=True)
            with open(f'Pasada_{step+1}/block_{curr_block}.json', 'w') as file:
                json.dump(merged_dict, file)

            curr_block += merge_size * 2

        if step > 0:
            shutil.rmtree(f'Pasada_{step}')

        merge_size *= 2
        step += 1

    if step > 0:
        with open(f'Pasada_{step}/block_0.json', 'r') as file:
            global_index = json.load(file)
        keys = sorted(global_index.keys())
        block_size = len(keys) // n_blocks
        for i in range(n_blocks):
            if i == n_blocks - 1:
                block_keys = keys[i*block_size:]
            else:
                block_keys = keys[i*block_size:(i+1)*block_size]
            block_dict = {key: global_index[key] for key in block_keys}
            os.makedirs('global_index', exist_ok=True)
            with open(f'global_index/block_{i}.json', 'w') as file:
                json.dump(block_dict, file)

        shutil.rmtree(f'Pasada_{step}')
```

1. Inicializaci√≥n de Variables:
    - **`total_blocks`**: Representa el n√∫mero total de bloques que se fusionar√°n.
    - **`merge_size`**: Representa el tama√±o de la fusi√≥n actual.
    - **`step`**: Representa la etapa actual del proceso de fusi√≥n.
2. Bucle Principal:
    - La fusi√≥n se realiza en varios niveles (etapas). En cada nivel, los bloques se fusionan en pares hasta que se obtiene un solo bloque.
    - En cada nivel, se duplica el tama√±o de la fusi√≥n (**`merge_size *= 2`**).
    - En cada nivel, se realiza un bucle para fusionar los bloques actuales (**`curr_block`**) y los bloques adyacentes (**`next_block`**).
3. Fusi√≥n de Bloques:
    - Se abren los archivos JSON correspondientes a los bloques actuales y siguientes.
    - Se cargan los √≠ndices invertidos de los bloques en las variables **`index1`** e **`index2`**.
    - Se fusionan los √≠ndices combinando las listas de posteo para las mismas claves.
    - El resultado de la fusi√≥n se guarda en **`merged_dict`**.
4. Almacenamiento del Resultado de Fusi√≥n:
    - Se crea un nuevo directorio (**`Pasada_{step+1}`**) para almacenar los resultados de la fusi√≥n actual.
    - Se guarda el √≠ndice fusionado en un archivo JSON correspondiente al bloque actual.
5. Actualizaci√≥n de Variables:
    - Se actualiza **`curr_block`** para apuntar al siguiente par de bloques a fusionar.
    - El proceso se repite hasta que se han fusionado todos los bloques en un solo bloque.
6. Limpieza de Archivos Temporales:
    - Despu√©s de cada nivel de fusi√≥n, se elimina el directorio de la etapa anterior para liberar espacio.
7. Generaci√≥n del √çndice Global:
    - Despu√©s de la √∫ltima fusi√≥n, se crea el √≠ndice global combinando los bloques resultantes en un solo √≠ndice invertido.
8. Almacenamiento del √çndice Global:
    - El √≠ndice global se almacena en archivos JSON separados para cada bloque en el directorio **`global_index`**.
9. Limpieza Final:
    - Se eliminan los archivos temporales y directorios intermedios.

## Ejecuci√≥n √≥ptima de consultas aplicando Similitud de Coseno

La similitud de coseno es una medida de similitud entre dos vectores en un espacio multidimensional, utilizandose en la recuperaci√≥n de informaci√≥n para comparar la similitud entre documentos o t√©rminos.

```python
def dict_query(self, query):
    query= self.process_query(query)
    query_tf = {term: query.count(term) for term in query}
    return query_tf
```

- La funci√≥n **`process_query`** (que no est√° proporcionada en el c√≥digo) se utiliza para preprocesar la consulta.
- La consulta se convierte en un diccionario donde las claves son los t√©rminos y los valores son las frecuencias de cada t√©rmino en la consulta.

```python
def cosine_similarity(self, query_vector, doc_vectors, k):
    similarities = {}
    for doc_id, doc_vector in doc_vectors.items():
        doc_norm = np.linalg.norm(doc_vector)
        if doc_norm == 0:
            similarity =0
        else:
            similarity = np.dot(query_vector, doc_vector) / (np.linalg.norm(query_vector) * doc_norm)

        similarities[doc_id] = similarity
    # Ordenar los documentos por similitud
    sorted_docs = sorted(similarities.items(), key=lambda item: item[1], reverse=True)
    # Seleccionar los k documentos m√°s similares
    top_k_docs = [{'id': doc_id, 'score': score} for doc_id, score in sorted_docs[:k]]
    return top_k_docs
```

- Calcula la similitud coseno entre el vector de consulta y cada vector de documento.
- Los resultados se almacenan en un diccionario **`similarities`**, que se ordena seg√∫n las puntuaciones de similitud en orden descendente.
- Se seleccionan los primeros k documentos m√°s similares y se devuelven en un formato espec√≠fico.

```python
# Construir vectores de documentos y vector de consulta
def build_document_vectors(self, query):
    # Inicializar variables
    terms = list(set())
    files = os.listdir('global_index')
    doc_vectors = {}  # Nuevo diccionario para almacenar los vectores de los documentos
    
    # Iterar sobre archivos de √≠ndice invertido para construir t√©rminos √∫nicos
    for file in files:
        with open(os.path.join('global_index', file), 'r') as f:
            inverted_index = json.load(f)
            
            # Actualizar t√©rminos con los del √≠ndice invertido
            if inverted_index:
                if isinstance(inverted_index, list):
                    terms.update([item['term'] for w in inverted_index for item in w])
                else:
                    terms.update(inverted_index.keys())
    
    # Preparar lista de t√©rminos √∫nica y crear √≠ndice de t√©rminos
    terms = list(terms)
    terms.extend(query.keys())
    terms = sorted(set(terms))
    term_index = {term: index for index, term in enumerate(terms)}
    
    # Iterar sobre archivos de √≠ndice invertido para construir vectores de documentos
    for file in files:
        with open(os.path.join('global_index', file), 'r') as f:
            inverted_index = json.load(f)
            
            # Iterar sobre t√©rminos y documentos en el √≠ndice invertido
            for term, term_docs in inverted_index.items():
                # Definir 'term' para este contexto
                
                # Iterar sobre la informaci√≥n de documentos para el t√©rmino
                for doc_info in term_docs:
                    doc_id = doc_info['id']
                    
                    # Inicializar el vector del documento si no existe
                    if doc_id not in doc_vectors:
                        doc_vectors[doc_id] = np.zeros(len(terms))
                    
                    # Actualizar el vector del documento con la frecuencia del t√©rmino
                    tf = doc_info['tf']
                    doc_vectors[doc_id][term_index[term]] = tf
    
    # Actualizar los vectores de los documentos para que tengan la misma longitud que los t√©rminos
    for doc_id, vector in doc_vectors.items():
        if len(vector) < len(terms):
            doc_vectors[doc_id] = np.pad(vector, (0, len(terms) - len(vector)))
    
    # Inicializar vector de consulta con ceros y actualizar con frecuencias de t√©rminos en la consulta
    query_vector = np.zeros(len(terms))
    for term, tf in query.items():
        if term in inverted_index:
            df_t = len(inverted_index[term])
            if df_t == 0:
                df_t = 1
            idf_t = math.log(len(inverted_index) / df_t)
        
        if term in term_index:
            query_vector[term_index[term]] = tf
    
    return doc_vectors, query_vector

```

- `build_document_vectors()` construye vectores de documentos y un vector de consulta basados en un √≠ndice invertido. Se procesan los t√©rminos √∫nicos y se construye un √≠ndice de t√©rminos. Luego, se iteran sobre los archivos de √≠ndice invertido para actualizar los vectores de documentos con las frecuencias de t√©rminos. Finalmente, se asegura de que los vectores de documentos tengan la misma longitud que la lista de t√©rminos y se construye el vector de consulta con las frecuencias de t√©rminos presentes en la consulta.

```python
def show_top_k(self, query, k, dataset):
    doc_vectors, query_vector = self.build_document_vectors(query)
    top_k_docs = self.cosine_similarity(query_vector, doc_vectors, k)
    for i, doc in enumerate(top_k_docs, 1):
        doc_id = doc['id']
        score = doc['score']
        doc_data = dataset[dataset['track_id'] == doc_id]
        print(f"Top {i} Document:")
        print(doc_data)
        print(f"Score: {score}\n")
```

- Utiliza las funciones anteriores para obtener los k documentos m√°s similares a la consulta.
- Imprime informaci√≥n sobre estos documentos, incluidos los datos del conjunto de datos y las puntuaciones de similitud.

## ¬øC√≥mo se construye el √≠ndice invertido en PostgreSQL?

A grandes rasgos, para la construcci√≥n del √≠ndice invertido en PostgreSQL se necesitan 3 tablas principales.
- Una tabla para almacenar los documentos
- Una tabla para almacenar los t√©rminos
- Una tabla de relaci√≥n entre t√©rminos y documentos

A partir de esto, se requere extraer los t√©rminos de los documentos y almacenarlos en una tabla de t√©rminos. Por ejemplo, se tiene lo siguiente:
```sql
INSERT INTO terms (term)
SELECT DISTINCT unnest(string_to_array(lower(content), ' ')) AS term
FROM documents;
```

La funci√≥n string_to_array se utiliza para dividir el contenido de los documentos en t√©rminos individuales, y la funci√≥n unnest se utiliza para convertir el resultado en filas individuales.

Lo siguiente es armar la relaci√≥n entre los documentos y t√©rminos. Esto se puede lograr utilizando consultas SQL que identifiquen los t√©rminos que aparecen en cada documento y los almacenen en la tabla de relaci√≥n. Por ejemplo:
```sql
INSERT INTO document_term (document_id, term)
SELECT d.id, t.term
FROM documents d
JOIN terms t ON lower(d.content) LIKE '%' || t.term || '%';
```
Una vez que se han extra√≠do los t√©rminos y se ha creado la tabla de relaci√≥n, se pueden crear √≠ndices en las columnas relevantes para mejorar el rendimiento de las consultas de b√∫squeda. Por lo general, se crean √≠ndices en las columnas de t√©rminos y en las columnas de identificadores de documentos para acelerar las b√∫squedas.

Con el √≠ndice invertido ya constru√≠do se pueden realizar consultas de b√∫squeda utilizando cl√°usulas SQL como WHERE y JOIN. Estas consultas aprovechan los √≠ndices para buscar r√°pidamente los documentos que contienen los t√©rminos de b√∫squeda especificados. Por tal motivo, las b√∫squedas de texto se vuelven m√°s eficientes, ya que se evita la necesidad de realizar exploraciones completas de los documentos.


# Backend (√çndice Multidimensional)
- Extracci√≥n de caracter√≠sticas: 
Los vectores de caracter√≠sticas se almacenan en una base de datos PostgreSQL. Donde se realiza una consulta a la tabla `vectores` de la base de datos, y los resultados se almacenan en el diccionario `features`, donde las claves son los identificadores de las pistas (track_id) y los valores son los vectores de caracter√≠sticas (mfcc).

- C√≥digo para extraer caracter√≠sticas:

```python
query = feature_extraction("/Directory")
query_vector = query

```

## KNN Secuencial: (Sin indexaci√≥n)
El algoritmo KNN-secuencial es una implementaci√≥n b√°sica y directa del algoritmo de los k-vecinos m√°s cercanos (KNN) que realiza una b√∫squeda exhaustiva de los vecinos m√°s cercanos sin utilizar estructuras de datos especializadas para acelerar las consultas, s√≥lo utilizando una cola de prioridad b√°sica.

Para la b√∫squeda KNN-secuencial, no se implement√≥ ninguna t√©cnica con indexaci√≥n por lo que no mejora la eficiencia en las b√∫squedas. El algoritmo eval√∫a los puntos de datos en el conjunto para encontrar los `k` vecinos m√°s cercanos al punto de la consulta. En est√° t√©cnica calculamos la similitud que al ser sin indexaci√≥n conlleva una gran complejidad computacional. 
Se implement√≥ los siguientes algoritmos: `KnnSearch(Q, k)`, `RangeSearch(Q, r)` 

![knn_search](https://github.com/Diegospf12/CapibaraGrooveFinder/assets/91237434/f534b6b0-7b1b-4c5f-8f66-5d3ff1f27874)
.png)

- `KnnSearch(Q, k)`: Recibe el vector de consulta 'query_vector' y un entero 'k' que representa la cantidad de vecinos m√°s cercanos a buscar. Se realiza la consutla a la base de datos para obtener los vectores caracter√≠sticos de audio, luego calculamos la distancia euclidiana entre el vector de consulta y todos los vectores en la base de datos. Utilizamos `heapq' para encontrar los 'k' elementos m√°s peque√±os. Al final devuelve la lista de tuplas, con el identificador de audio y la distancia euclidiana. 

![KNN_RANGE](https://github.com/Diegospf12/CapibaraGrooveFinder/assets/91237434/82ba852a-d2fb-4364-bced-6f10e3276f17)



- `Range_Search`: Al igual que el KnnSearch, recibe el vector de consulta 'query_vector' pero con el valor del radio 'radius'. Se realiza la b√∫squeda en la base de datos para obtener los vectores caracteristicos, se selecciona de la tabla 'songs'. Realiza el c√°lculo de la distancia euclidiana, filtra las distancias para incluir solo aquellos vectores que estan en el radio de b√∫squeda. Al final devuelve una lista de tuplas que contiene la distancia euclidiana y el 'track'.

## KNN RTree

Para que la b√∫squeda sea m√°s eficiente, hacemos uso del `RTree` como una libreria de √≠ndice espacial para indexar todos los vectores caracter√≠sticos. La b√∫squeda RTree en el KNN donde el rango es inicialmente infinito, luego se va reduciendo el rango seg√∫n se van evaluando. 

![R-Tree](https://github.com/Diegospf12/CapibaraGrooveFinder/assets/91237434/a00a2664-b629-42ff-8a65-4f875f457963)


- `KNN RTree`: 

Ejecutamos una consulta SQL para obtener los identificadores de las pistas 'track_id' y sus vectroes caracter√≠ticos 'mffcc' desde la tabla 'vectores'. Las caracter√≠ticas se almancenan en el dict 'features'. Para la construcci√≥n del √≠ndice R-tree, utilizamos el m√≥dulo rtree para crear el √≠ndice, lo configuramos con la propiedad de 'p.dimension'. Se insertan los objetos en el √≠ndice RTree utlizandos los identificadores de las pistas y los vectores caracter√≠ticos concatenados.
La funci√≥n `knn_search_rtree` va a recibir un vector de consulta 'query_vector' y un entro 'k' que representa la cantdiad de vecinos m√°s cercanos. Calculamos la distancia euclidiana entre el vector de consulta y los objetos encontrados en el √≠ndice. Al final devolvemos una lista de tuplas, donde cada tupla contiene el identificador del audio y la distancia euclidiana correspondiente. 


## KNN HighD

El algoritmo KNN en espacios de alta dimensi√≥n (High D) es un m√©todo de aprendizaje supervisado que se utiliza para clasificaci√≥n y regresi√≥n. Dado un conjunto de datos de entrenamiento con etiquetas conocidas, el algoritmo KNN clasifica nuevos ejemplos en funci√≥n de la similitud de sus caracter√≠sticas con las de los ejemplos de entrenamiento.


### **¬øQu√© la maldici√≥n de la dimensionalidad?**

La efectividad del algoritmo KNN disminuye en entornos de alta dimensionalidad debido a desaf√≠os espec√≠ficos. 

En un espacio de alta dimensi√≥n, la eficiencia del algoritmo KNN puede verse afectada debido a la "maldici√≥n de la dimensionalidad". Esto se refiere al fen√≥meno en el que el volumen del espacio de caracter√≠sticas aumenta tan r√°pidamente con el aumento de la dimensionalidad que los datos se vuelven escasos y dispersos. A medida que se incrementa el n√∫mero de caracter√≠sticas, los c√°lculos de distancias y la identificaci√≥n de vecinos cercanos se vuelven m√°s costosos computacionalmente. La premisa de que puntos similares est√°n cercanos se vuelve menos v√°lida, ya que las distancias entre puntos pierden su distintividad. 

Para superar este problema, se utilizan t√©cnicas de reducci√≥n de la dimensionalidad o usos de m√©todos de indexaci√≥n eficientes. En nuestro caso, aplicamos el FAISS de Facebook para acelerar las consultas.

![An√°lisis1](https://github.com/Diegospf12/CapibaraGrooveFinder/assets/91237434/2afb3f3c-4994-4c3b-98ee-15298d10da33)

## An√°lisis de la maldici√≥n de la dimensionalidad y como mitigarlo

![An√°lisis](https://github.com/Diegospf12/CapibaraGrooveFinder/assets/91237434/22053932-3678-4d87-aa29-186f2a643ef0)


### **¬øC√≥mo mitigarlo?**

**`Reducci√≥n de Datos: `**

Aborda la maldici√≥n de la dimensionalidad utilizando t√©cnicas como selecci√≥n y extracci√≥n de funciones (PCA, kernel) para conservar la relevancia en espacios de baja dimensi√≥n.

**`K-NN Aproximado`**

Cuando la b√∫squeda exacta se vuelve costosa, se opta por algoritmos de k-NN aproximado. El hash sensible a la localidad (LSH) divide elementos similares en dep√≥sitos de alta probabilidad, ofreciendo una soluci√≥n r√°pida para k-NN en entornos de alta dimensi√≥n. Proyecciones aleatorias y bosques de proyecci√≥n aleatoria (rpForests) permiten transformar datos y realizar agrupaciones aproximadas de manera eficiente para encontrar vecinos m√°s cercanos.

### Faiss (Facebook AI Similarity Search) 

Es conocida por su eficacia en la b√∫squeda de vecinos m√°s cercanos y la b√∫squeda de similitud en espacios vectoriales. En este caso utlizaremos el √≠ndice de Faiss **`IndexIVFFlat`** que utilza una t√©cnica llamada Inverted File Indexing (IVF) y utiliza el cuantizador plano **`IndexFlatL2`** para cuantificar los vectores en las celdas del √≠ndice IVF.


![faiss](https://github.com/Diegospf12/CapibaraGrooveFinder/assets/91237434/0a87f89b-3335-48fe-90e2-d24b2b5a7783)

![ejem](https://github.com/Diegospf12/CapibaraGrooveFinder/assets/91237434/f722da10-0132-4b48-bb34-51c8395cb865)


**Creaci√≥n del √≠ndice Faiss:**

```python
quantizer = faiss.IndexFlatL2(feature_matrix.shape[1])
index = faiss.IndexIVFFlat(quantizer, feature_matrix.shape[1], 3)
```
- Se utiliza **`faiss.IndexIVFFlat`** como √≠ndice Inverted File Index (IVF) plano. Este √≠ndice segmenta el espacio de b√∫squeda en celdas, organizando una lista invertida para cada celda. La construcci√≥n de listas invertidas facilita la b√∫squeda eficiente en conjuntos de datos de alta dimensionalidad, ya que reduce el espacio de b√∫squeda a celdas espec√≠ficas, mejorando significativamente la velocidad de recuperaci√≥n de vecinos m√°s cercanos. Difinimos a 'nprobe' (Cuantos `centroides` se exploran durante la b√∫suqeda) con 3 pero se puede mejorar seg√∫n la presici√≥n de la b√∫squeda y la velocidad.  

- Se utiliza **`faiss.IndexFlatL2`** como cuantizador plano. Este componente se encarga de asignar los vectores de caracter√≠sticas a celdas espec√≠ficas en funci√≥n de su proximidad, utilizando la distancia euclidiana como medida de proximidad. Esta elecci√≥n de cuantizador permite una r√°pida cuantificaci√≥n de los vectores para su posterior indexaci√≥n.


**Adici√≥n de datos al √≠ndice:**

```python
index.train(feature_matrix)
index.add(feature_matrix)
```
Se entrena el `√≠ndice Faiss` con la matriz de caracter√≠sticas utilizando el m√©todo 'train'. Luego se a√±aden los vectores de caracter√≠sticas al √≠ndice utilizando el m√©todo 'add'.

**Lectura y escritura del √≠ndice:**

```python
faiss.write_index(index, 'index_file')
```
- Utilizando el m√©todo write_index.

**La b√∫squeda de vecinos m√°s cercanos:**
```python
distances, indices = index.search(query_matrix, K)
```
- Se realiza una b√∫squeda de k vecinos m√°s cercanos en el √≠ndice Faiss utilizando el vector de consulta 'query_vector'. Devuelve una lista de tuplas, donde cada tupla contiene el identificador de la pista de audio (audio_path) y la distancia asociada.
  
![index2](https://github.com/Diegospf12/CapibaraGrooveFinder/assets/91237434/05d19b20-dca9-4079-8613-77c525fc7b5c)

![index](https://github.com/Diegospf12/CapibaraGrooveFinder/assets/91237434/e7be27f1-a093-45ec-a533-122459d36f6e)

- **Desventajas**
    -  Consumo de memoria: La creaci√≥n de √≠ndices IVF puede requerir m√°s memoria, siendo una limitaci√≥n en conjuntos de datos grandes o en entornos con recursos limitados.
    - Entrenamiento: Antes de las b√∫squedas, el √≠ndice IVF necesita ser entrenado con datos de entrenamiento, a√±adiendo un paso computacionalmente costosos, especialmente para conjuntos de datos extensos.
    - Tama√±o del √≠ndice: Los √≠ndices IVF tienden a ser m√°s grandes en comparaci√≥n con √≠ndices m√°s simples, lo que puede ser cr√≠tico si el espacio de almacenamiento es limitado.



`Fuentes:`
- https://www.baeldung.com/cs/k-nearest-neighbors
- https://es.quora.com/Qu%C3%A9-es-la-maldici%C3%B3n-de-la-dimensionalidad
- https://www.youtube.com/watch?v=nhgCwWGNDiM


# Frontend (GUI)

## Mini-manual de usuarios
### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/Diegospf12/CapibaraGrooveFinder
cd CapibaraGrooveFinder
```

### Paso 2: Configuraci√≥n del Backend

Instalar dependencias
```bash
cd backend
source venv/bin/activate
pip install -r requirements.txt
```

Ejecutar servidor
```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### Paso 3: Configuraci√≥n del Frontend

Instalar dependencias
```bash
cd frontend
npm install
npm run serve
```

## Screenshots
![[Main page]](https://github.com/Diegospf12/CapibaraGrooveFinder/blob/main/images/main.png)
![[Lyrics page]](https://github.com/Diegospf12/CapibaraGrooveFinder/blob/main/images/lyrics.png)
![[Similar page]](https://github.com/Diegospf12/CapibaraGrooveFinder/blob/main/images/similarpage.png)

## An√°lisis comparativo visual con otras implementaciones
![[Similar page]](https://github.com/Diegospf12/CapibaraGrooveFinder/blob/main/images/example1.png)
![[Similar page]](https://github.com/Diegospf12/CapibaraGrooveFinder/blob/main/images/example2.png)

# Experimentaci√≥n

## Tablas y gr√°ficos de los resultados experimentales

|      | Implementaci√≥n | Postgress |
|------|----------------|-----------|
| 1000 |     15 ms      |   19 ms   |
| 2000 |     34 ms      |   33 ms   |
| 4000 |     65 ms      |   72 ms   |
| 8000 |    190 ms      |  180 ms   |
|16000 |    295 ms      |  300 ms   |
|32000 |    333 ms      |  359 ms   |
|64000 |    380 ms      |  360 ms   |


### Implementaci√≥n de √çndice Invertido

Nuestra implementaci√≥n personalizada de √≠ndice invertido ha demostrado ser eficiente en la b√∫squeda de datos basada en texto. Hemos medido el rendimiento de nuestra implementaci√≥n en milisegundos para una variedad de tama√±os de datos de prueba, desde 1,000 hasta 64,000 registros. 

### PostgreSQL con √çndice Invertido

Por otro lado, hemos utilizado PostgreSQL con sus propios √≠ndices invertidos para realizar b√∫squedas de texto similares a nuestras pruebas con datos de prueba de tama√±os comparables.

### Gr√°fico comparativo

![graph1](https://github.com/Diegospf12/CapibaraGrooveFinder/blob/main/images/grafico_comparativo_1.png)

## Tabla de resultados de √≠ndices multidimensionales

Ejecutamos el KNN-RTree, KNN-secuencial y el KNN-HighD sobre una colecci√≥n de objetos de tama√±o N y comparamos la eficiencia en funci√≥n del tiempo de ejecuci√≥n

|  k=8 | Secuencial |  KNN-RTree  |  KNN-HighD  |
|------|------------|-------------|-------------|
| 1000 | 0.02148 ms | 0.001276 ms | 0.011387 ms |
| 2000 | 0.03314 ms | 0.002307 ms | 0.013839 ms |
| 4000 | 0.04983 ms | 0.006453 ms | 0.013992 ms |
| 8000 | 0.09919 ms | 0.015350 ms | 0.015020 ms |
|12000 | 0.16726 ms | 0.026420 ms | 0.015874 ms |
|16000 | 0.17850 ms | 0.035708 ms | 0.015996 ms |

### Gr√°fico comparativo de tiempos

![graph2](https://github.com/Diegospf12/CapibaraGrooveFinder/blob/main/images/grafico_comparativo_2.png)


### An√°lisis y Discusi√≥n
- Se puede ver que el √≠ndice IVFFlat de Faiss es el m√©todo m√°s √≥ptimo para la b√∫squeda por similitud ya que es el que mejor se mantiene mientras m√°s vectores aparecen en la colecci√≥n.

- La b√∫squeda KNN con Rtree empieza siendo la mejor cuando hay pocos vectores, pero a medida que la cantidad de vectores van incrementando, se va haciendo m√°s ineficiente.

- La b√∫squeda secuencial es la menos eficiente de todas, se puede notar que su crecimiento a medida que van apareciendo m√°s vectores es exponencial.

- Nuestra implementaci√≥n de √≠ndice invertido a la hora de hacer el filtrado pierda algunos t√©rminos ya que estos pueden estar concatenados con s√≠mbolos raros (".!-) y esto puede ocacionar que la b√∫squeda de ciertos documentos que contienen estas palabras no sean exactos.

### Conclusiones

Los resultados de nuestra comparaci√≥n indican que PostgreSQL supera ligeramente a nuestra implementaci√≥n personalizada de √≠ndice invertido en t√©rminos de rendimiento en la mayor√≠a de los escenarios. Aunque nuestra implementaci√≥n es eficiente, PostgreSQL, con su optimizaci√≥n interna y capacidad para manejar grandes conjuntos de datos, muestra tiempos de b√∫squeda ligeramente m√°s bajos en las pruebas. Por otro lado a la hora de b√∫squeda por similitud de audio, podemos darnos cuenta que la mejor opci√≥n fue el √çndice IVFFlat de Faiss ya que esye divide los audios por cl√∫sters, para asi reducir la cantidad de comparaciones.


### Referencias bibliogr√°ficas



- A. Guttman. R-trees: A dynamic index structure for spatial searching. In Proc. ACM International Conference on Management of Data (SIGMOD‚Äô84), pages 47‚Äî57. ACM Press 1984
- Baeldung. (s.f.). K-Nearest Neighbors (KNN) Algorithm in C#. Baeldung. https://www.baeldung.com/cs/k-nearest-neighbors
- Facebook Research. (A√±o). Faiss: A library for efficient similarity search and clustering. Recuperado de https://github.com/facebookresearch/faiss
- FAISS. (s.f.). IndexIVFFlat. Faiss. https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexIVFFlat.html
- Pinecone. Faiss Tutorial. Pinecone Learn. https://www.pinecone.io/learn/series/faiss/faiss-tutorial/

### Tareas asignadas

![tasks](https://github.com/Diegospf12/CapibaraGrooveFinder/blob/main/images/tasks.png)

